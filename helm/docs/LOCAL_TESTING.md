# Tests Locaux du Helm Chart avec K3s

Ce guide explique comment tester le Helm chart Starlake localement avec K3s (Kubernetes l√©ger).

## Pourquoi K3s ?

- ‚úÖ **L√©ger** : 50 Mo vs 500+ Mo pour Minikube
- ‚úÖ **Rapide** : D√©marre en quelques secondes
- ‚úÖ **Complet** : Support Ingress, LoadBalancer (via Traefik), storage local
- ‚úÖ **Production-like** : Architecture identique √† un vrai cluster
- ‚úÖ **Multi-plateforme** : macOS, Linux, Windows (WSL2)

## Installation de K3s

### macOS / Linux

```bash
# Installer k3s via k3d (K3s in Docker - plus simple sur macOS)
brew install k3d

# Ou t√©l√©charger directement
# curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
```

### Windows (WSL2)

```bash
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
```

### V√©rifier l'installation

```bash
k3d version
# Exemple de sortie: k3d version v5.6.0
```

## Cr√©ation du Cluster de Test

### Option 1 : Cluster Basique

```bash
# Cr√©er un cluster K3s simple
k3d cluster create starlake-test \
  --agents 2 \
  --port "8080:80@loadbalancer" \
  --port "8443:443@loadbalancer"

# V√©rifier que kubectl est configur√©
kubectl cluster-info
kubectl get nodes
```

### Option 2 : Cluster avec Configuration Avanc√©e

```bash
# Cr√©er un cluster avec plus de ressources
k3d cluster create starlake-test \
  --agents 3 \
  --servers 1 \
  --port "8080:80@loadbalancer" \
  --port "8443:443@loadbalancer" \
  --k3s-arg "--disable=traefik@server:0" \
  --volume "$(pwd)/projects:/projects@all"

# Note: On d√©sactive Traefik si on veut utiliser NGINX Ingress
```

## Multi-Node Clusters and local-path Storage Limitations

When using K3d with multiple nodes and the default `local-path` storage class, there are important limitations to understand.

### The Problem

The `local-path` storage provisioner in K3s has the following behavior:

1. **Node Affinity**: PersistentVolumes are created with node affinity - the volume is physically stored on one specific node
2. **First Consumer Binding**: The PV binds to whichever node first creates a pod that uses the PVC
3. **No Cross-Node Access**: Pods on other nodes cannot access the volume

This creates issues in multi-node clusters:

```
Example scenario:
- PVC is created and bound to agent-0 (first pod scheduled there)
- Gizmo pod with hostNetwork needs to run on server-0 (where ports are mapped)
- Gizmo cannot start because the PVC is only accessible on agent-0
```

### K3d Port Mapping and hostNetwork

K3d port mapping (e.g., `--port "11900:11900@server:0"`) forwards traffic from the host to a specific node:

- `@server:0` - Forward to the first server node
- `@loadbalancer` - Forward to the built-in load balancer (for HTTP/HTTPS)
- `@agent:0` - Forward to the first agent node

When a service uses `hostNetwork: true` (like Gizmo for Arrow Flight SQL), the pod must run on the node where the port is mapped. But if the PVC is bound to a different node, there's a conflict.

### Solutions

#### Solution 1: Single-Node Cluster (Recommended for Development)

The simplest solution is to use a single-node cluster where there's no node affinity conflict:

```bash
k3d cluster create starlake-test \
  --servers 1 \
  --agents 0 \
  --port "8080:80@loadbalancer" \
  --port "11900-11920:11900-11920@server:0"
```

This is the recommended approach for local development and testing.

#### Solution 2: Use port-forward for Gizmo

In multi-node clusters, use `kubectl port-forward` instead of hostNetwork port mapping:

```bash
# Start the cluster without Gizmo-specific port mappings
k3d cluster create starlake-test --servers 1 --agents 2 --port "8080:80@loadbalancer"

# After deployment, port-forward to Gizmo
kubectl port-forward deploy/starlake-gizmo 11900:11900 -n starlake
```

#### Solution 3: RWX Storage (Production Approach)

For production or production-like testing, use a storage class that supports ReadWriteMany:

- **NFS Provisioner**: Works in any environment
- **AWS EFS**: For EKS clusters
- **GCP Filestore**: For GKE clusters
- **Azure Files**: For AKS clusters

With RWX storage, any pod on any node can access the volume.

### Gizmo Connection Details

When using port-forward or hostNetwork, connect to Gizmo using:

```
JDBC URL: jdbc:arrow-flight-sql://localhost:11900?useEncryption=true&disableCertificateVerification=true
Username: gizmosql_user
Password: gizmosql_password
```

For DBeaver or other SQL clients:
1. Install the Arrow Flight SQL JDBC driver
2. Use the connection URL above
3. Enable SSL but disable certificate verification (for development)

### Summary Table

| Cluster Type | Storage | Gizmo Access Method | Complexity |
|-------------|---------|---------------------|------------|
| Single-node K3d | local-path | hostNetwork (direct) | Simple |
| Multi-node K3d | local-path | port-forward | Medium |
| Multi-node K3d | NFS | hostNetwork (direct) | Medium |
| Production (EKS/GKE/AKS) | EFS/Filestore/Azure Files | Ingress or LoadBalancer | Production-ready |

## Installation des Pr√©requis

### 1. Installer Helm (si pas d√©j√† fait)

```bash
# macOS
brew install helm

# Linux
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# V√©rifier
helm version
```

### 2. Installer un Storage Provisioner (pour ReadWriteMany)

K3s inclut local-path-provisioner par d√©faut (ReadWriteOnce uniquement).
Pour ReadWriteMany, on installe NFS provisioner :

```bash
# Ajouter le repo Helm
helm repo add nfs-subdir-external-provisioner \
  https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/

# Pour K3s, on utilise un NFS server local
# Option A : Installer NFS server sur l'h√¥te (recommand√©)

# macOS (NFS est d√©j√† int√©gr√©, juste besoin de le configurer)
sudo mkdir -p /System/Volumes/Data/nfs/starlake-projects
# Ajouter √† /etc/exports:
echo "/System/Volumes/Data/nfs/starlake-projects -alldirs -mapall=$(id -u):$(id -g) localhost" | sudo tee -a /etc/exports
# Red√©marrer NFS
sudo nfsd restart

# Linux
sudo apt-get install nfs-kernel-server
sudo mkdir -p /srv/nfs/starlake-projects
sudo chown nobody:nogroup /srv/nfs/starlake-projects
echo "/srv/nfs/starlake-projects *(rw,sync,no_subtree_check,no_root_squash)" | sudo tee -a /etc/exports
sudo systemctl restart nfs-kernel-server

# Option B : Utiliser le provisioner local de K3s avec un workaround
# (moins id√©al mais fonctionne pour les tests)
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path-rwx
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
parameters:
  pathPattern: "/projects"
EOF
```

### 3. Installer NFS Provisioner (si NFS server disponible)

```bash
# Installer le provisioner avec l'IP de votre machine
# Obtenir l'IP locale
export HOST_IP=$(hostname -I | awk '{print $1}')  # Linux
export HOST_IP=$(ipconfig getifaddr en0)          # macOS

helm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --namespace kube-system \
  --set nfs.server=$HOST_IP \
  --set nfs.path=/srv/nfs/starlake-projects \
  --set storageClass.name=nfs-client \
  --set storageClass.defaultClass=false

# V√©rifier
kubectl get storageclass
```

## Test du Helm Chart

### √âtape 1 : Valider le Chart Localement

```bash
cd /path/to/starlake-data-stack

# Lint le chart
helm lint ./helm/starlake

# Dry-run pour voir les manifests g√©n√©r√©s
helm install starlake-test ./helm/starlake \
  --namespace starlake \
  --create-namespace \
  --dry-run --debug > /tmp/starlake-manifests.yaml

# V√©rifier le fichier g√©n√©r√©
less /tmp/starlake-manifests.yaml
```

### √âtape 2 : D√©ployer avec PostgreSQL Interne

```bash
# Cr√©er le namespace
kubectl create namespace starlake

# Installer avec configuration de test
helm install starlake ./helm/starlake \
  --namespace starlake \
  --set postgresql.internal.persistence.size=2Gi \
  --set persistence.projects.size=5Gi \
  --set persistence.projects.storageClass=nfs-client \
  --set ui.resources.requests.memory=256Mi \
  --set ui.resources.limits.memory=1Gi \
  --set airflow.webserver.resources.requests.memory=256Mi \
  --set airflow.webserver.resources.limits.memory=1Gi \
  --set agent.resources.requests.memory=128Mi \
  --set agent.resources.limits.memory=512Mi

# Alternative : Utiliser le fichier values-development.yaml
helm install starlake ./helm/starlake \
  --namespace starlake \
  --values ./helm/starlake/values-development.yaml \
  --set persistence.projects.storageClass=nfs-client
```

### √âtape 3 : Suivre le D√©ploiement

```bash
# Voir les pods en cours de cr√©ation
kubectl get pods -n starlake -w

# Voir les logs d'un pod sp√©cifique
kubectl logs -n starlake -l app.kubernetes.io/component=postgresql -f

# Voir tous les √©v√©nements
kubectl get events -n starlake --sort-by='.lastTimestamp'

# V√©rifier les PVCs
kubectl get pvc -n starlake
```

### √âtape 4 : Tester l'Acc√®s

#### Option A : Port-Forward (Simple et Recommand√©)

```bash
# Port-forward vers l'UI (point d'entr√©e principal)
# L'UI proxie automatiquement /airflow vers le service Airflow interne
kubectl port-forward svc/starlake-ui 8080:80 -n starlake

# Ouvrir dans le navigateur
open http://localhost:8080          # UI Starlake
open http://localhost:8080/airflow  # Airflow (via proxy UI)

# Credentials Airflow par d√©faut : airflow / airflow
```

> **Note** : L'UI agit comme reverse proxy pour Airflow. Un seul port-forward suffit pour acc√©der aux deux services sur le m√™me port.

#### Option B : LoadBalancer (K3s inclut un LoadBalancer)

```bash
# Obtenir l'IP externe (sera localhost ou 127.0.0.1)
kubectl get svc starlake-ui -n starlake

# Acc√©der via le port mapp√© lors de la cr√©ation du cluster
open http://localhost:8080
```

### √âtape 5 : Tests Fonctionnels

```bash
# 1. Tester la connexion PostgreSQL
kubectl exec -it starlake-postgresql-0 -n starlake -- \
  psql -U dbuser -d starlake -c "SELECT version();"

# 2. V√©rifier les bases de donn√©es
kubectl exec -it starlake-postgresql-0 -n starlake -- \
  psql -U dbuser -c "\l"

# 3. Tester la connectivit√© UI ‚Üí PostgreSQL
kubectl exec -it deployment/starlake-ui -n starlake -- \
  nc -zv starlake-postgresql 5432

# 4. V√©rifier les health checks
kubectl get pods -n starlake -o wide
kubectl describe pod starlake-ui-xxxxx -n starlake | grep -A 10 "Liveness\|Readiness"

# 5. Tester les API (avec port-forward sur 8080)
# Health check UI
curl http://localhost:8080/api/v1/health

# Health check Airflow (via proxy UI)
curl http://localhost:8080/airflow/health
```

### √âtape 6 : Test avec PostgreSQL Externe (Simulation)

```bash
# Cr√©er un PostgreSQL externe dans le cluster (pour simuler RDS)
kubectl run postgres-external \
  --image=postgres:17 \
  --env="POSTGRES_PASSWORD=external123" \
  --env="POSTGRES_USER=externaluser" \
  --env="POSTGRES_DB=starlake" \
  -n starlake

# Exposer comme service
kubectl expose pod postgres-external \
  --port=5432 \
  --name=postgres-external \
  -n starlake

# Attendre que le pod soit pr√™t
kubectl wait --for=condition=ready pod/postgres-external -n starlake --timeout=60s

# R√©installer Starlake avec PostgreSQL externe
helm uninstall starlake -n starlake

helm install starlake ./helm/starlake \
  --namespace starlake \
  --set postgresql.external.enabled=true \
  --set postgresql.external.host=postgres-external \
  --set postgresql.internal.enabled=false \
  --set postgresql.credentials.username=externaluser \
  --set postgresql.credentials.password=external123 \
  --set persistence.projects.storageClass=nfs-client
```

## Tests de Mise √† Jour (Upgrade)

```bash
# Modifier une valeur (ex: changer le nombre de replicas UI)
helm upgrade starlake ./helm/starlake \
  --namespace starlake \
  --reuse-values \
  --set ui.replicas=2

# Voir l'historique
helm history starlake -n starlake

# Rollback si n√©cessaire
helm rollback starlake -n starlake
```

## Tests de Performance (Optionnel)

```bash
# Stress test simple sur l'UI
kubectl run -it --rm load-test \
  --image=busybox \
  --restart=Never \
  -- sh -c 'while true; do wget -q -O- http://starlake-ui.starlake.svc/api/v1/health; done'

# Voir l'utilisation des ressources
kubectl top pods -n starlake
kubectl top nodes
```

## Checklist de Tests

- [ ] PostgreSQL d√©marre et est accessible
- [ ] UI d√©marre et se connecte √† PostgreSQL
- [ ] Airflow d√©marre (webserver + scheduler)
- [ ] Agent d√©marre
- [ ] Health checks passent pour tous les pods
- [ ] PVC projects est cr√©√© avec ReadWriteMany
- [ ] Logs sont accessibles via `kubectl logs`
- [ ] Port-forward fonctionne
- [ ] LoadBalancer fonctionne (si configur√©)
- [ ] Upgrade/rollback fonctionnent
- [ ] PostgreSQL externe fonctionne (test de simulation)

## Nettoyage

```bash
# Supprimer le release Helm
helm uninstall starlake -n starlake

# Supprimer les PVCs (optionnel)
kubectl delete pvc -l app.kubernetes.io/instance=starlake -n starlake

# Supprimer le namespace
kubectl delete namespace starlake

# Supprimer le cluster K3s
k3d cluster delete starlake-test
```

## D√©pannage

### Pods en CrashLoopBackOff

```bash
# Voir les logs du pod
kubectl logs <pod-name> -n starlake --previous

# D√©crire le pod pour voir les events
kubectl describe pod <pod-name> -n starlake
```

### PVC en Pending

```bash
# V√©rifier le PVC
kubectl describe pvc starlake-projects -n starlake

# V√©rifier les storage classes
kubectl get storageclass

# Si NFS ne fonctionne pas, utiliser local-path pour les tests
helm upgrade starlake ./helm/starlake \
  --namespace starlake \
  --set persistence.projects.storageClass=local-path \
  --set persistence.projects.size=2Gi
```

### PostgreSQL ne d√©marre pas

```bash
# V√©rifier les logs
kubectl logs starlake-postgresql-0 -n starlake

# V√©rifier le PVC
kubectl get pvc -n starlake | grep postgresql

# Supprimer et recr√©er
helm uninstall starlake -n starlake
kubectl delete pvc data-starlake-postgresql-0 -n starlake
helm install starlake ./helm/starlake --namespace starlake
```

## Automatisation des Tests

Cr√©er un script de test automatis√© :

```bash
#!/bin/bash
# test-helm-chart.sh

set -e

echo "üß™ Test du Helm Chart Starlake"

# 1. Cr√©er le cluster
echo "üì¶ Cr√©ation du cluster K3s..."
k3d cluster create starlake-test --agents 2 --port "8080:80@loadbalancer"

# 2. Installer le chart
echo "üöÄ Installation du chart..."
helm install starlake ./helm/starlake \
  --namespace starlake \
  --create-namespace \
  --values ./helm/starlake/values-development.yaml \
  --wait --timeout 10m

# 3. V√©rifier les pods
echo "‚úÖ V√©rification des pods..."
kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=starlake -n starlake --timeout=5m

# 4. Tests fonctionnels
echo "üîç Tests fonctionnels..."

# Test PostgreSQL
kubectl exec starlake-postgresql-0 -n starlake -- psql -U dbuser -d starlake -c "SELECT 1" > /dev/null
echo "  ‚úì PostgreSQL OK"

# Test UI health
kubectl port-forward svc/starlake-ui 8080:80 -n starlake &
sleep 5
curl -f http://localhost:8080/api/v1/health > /dev/null
echo "  ‚úì UI Health OK"
kill %1

# 5. Nettoyage
echo "üßπ Nettoyage..."
helm uninstall starlake -n starlake
k3d cluster delete starlake-test

echo "‚úÖ Tous les tests ont r√©ussi !"
```

Rendre le script ex√©cutable :
```bash
chmod +x helm/test-helm-chart.sh
./helm/test-helm-chart.sh
```

## Int√©gration Continue (CI)

Exemple de GitHub Actions workflow :

```yaml
# .github/workflows/test-helm.yml
name: Test Helm Chart

on:
  pull_request:
    paths:
      - 'helm/**'
  push:
    branches:
      - main

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install k3d
        run: curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash

      - name: Install Helm
        uses: azure/setup-helm@v3

      - name: Create K3s cluster
        run: k3d cluster create test --agents 2

      - name: Lint Helm chart
        run: helm lint ./helm/starlake

      - name: Install chart
        run: |
          helm install starlake ./helm/starlake \
            --namespace starlake \
            --create-namespace \
            --values ./helm/starlake/values-development.yaml \
            --wait --timeout 10m

      - name: Test pods are running
        run: |
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/instance=starlake \
            -n starlake --timeout=5m

      - name: Cleanup
        if: always()
        run: k3d cluster delete test
```

## Prochaines √âtapes

Apr√®s validation locale avec K3s :

1. **Tester sur un vrai cluster** (EKS, GKE, AKS)
2. **Configurer monitoring** (Prometheus, Grafana)
3. **Mettre en place CI/CD** (ArgoCD, Flux)
4. **Documenter les cas d'usage production**
5. **Publier le chart** (sur un Helm repository)
