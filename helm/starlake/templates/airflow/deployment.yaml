{{- if .Values.airflow.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "starlake.fullname" . }}-airflow
  labels:
    {{- include "starlake.componentLabels" (dict "component" "airflow" "context" .) | nindent 4 }}
spec:
  replicas: {{ .Values.airflow.webserver.replicas }}
  selector:
    matchLabels:
      {{- include "starlake.componentSelectorLabels" (dict "component" "airflow" "context" .) | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "starlake.componentSelectorLabels" (dict "component" "airflow" "context" .) | nindent 8 }}
    spec:
      serviceAccountName: {{ include "starlake.serviceAccountName" . }}
      {{- if .Values.airflow.jobRunner.enabled }}
      # Required for Job Runner: ensures SA token is mounted for kubectl auth
      automountServiceAccountToken: true
      {{- end }}
      # fsGroup ensures shared volume files are accessible by all pods
      # Airflow runs as user 50000, but fsGroup adds group 1000 for shared access
      securityContext:
        fsGroup: {{ .Values.podSecurityContext.fsGroup }}
      initContainers:
        {{- include "starlake.waitForPostgresql" . | nindent 8 }}
        # Fix permissions on existing files - differentiated for security
        # Most files: group read/write (g+rwX) for shared access
        # DuckDB stored_secrets: group read only (g-w) to protect credentials
        - name: fix-permissions
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              echo "Fixing permissions on /projects for group {{ .Values.podSecurityContext.fsGroup }}..."

              # Set group ownership on all files
              chgrp -R {{ .Values.podSecurityContext.fsGroup }} /projects 2>/dev/null || true

              # Default: group read/write for all files
              chmod -R g+rwX /projects 2>/dev/null || true

              # DuckDB stored_secrets directories: remove group write (contains unencrypted credentials)
              find /projects -type d -name "stored_secrets" -exec chmod -R g-w {} \; 2>/dev/null || true

              # DuckDB secret files: owner-only (DuckDB requires 0600 for security)
              find /projects -name "*.duckdb_secret" -exec chmod 600 {} \; 2>/dev/null || true

              echo "Permissions fixed (stored_secrets: g-w, .duckdb_secret: 600, rest: g+rwX)"
              ls -la /projects/
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: projects
              mountPath: /projects
        {{- if .Values.airflow.installPythonPackages }}
        # Copy Starlake CLI from UI image (only needed when using official Airflow image)
        # When using custom image (Dockerfile_airflow), starlake is already installed
        - name: setup-starlake-tools
          image: "{{ .Values.ui.image.repository }}:{{ .Values.ui.image.tag }}"
          imagePullPolicy: {{ .Values.ui.image.pullPolicy }}
          command:
            - /bin/bash
            - -c
            - |
              echo "=== Setting up Starlake tools for Airflow ==="

              # Copy entire Starlake directory (contains starlake.sh and lib/)
              echo "Copying Starlake CLI directory..."
              mkdir -p /shared-tools/starlake
              if [ -d "/app/starlake" ]; then
                cp -r /app/starlake/* /shared-tools/starlake/ 2>/dev/null || true
                echo "Copied from /app/starlake"
                ls -la /shared-tools/starlake/
              fi

              # Copy gcloud SDK if available in UI image
              if [ -d "/usr/local/gcloud" ]; then
                echo "Copying Google Cloud SDK..."
                cp -r /usr/local/gcloud /shared-tools/
              fi

              # Create wrapper scripts in bin directory
              mkdir -p /shared-tools/bin

              # Create a shell-mode starlake wrapper that executes locally (not via docker exec)
              # This wrapper handles --options and calls the actual starlake.sh
              cat > /shared-tools/bin/starlake << 'EOFSL'
              #!/usr/bin/env bash
              old_ifs="$IFS"

              if [ "$#" -eq 0 ]; then
                echo "No arguments provided. Usage: starlake <command> [args...]"
                exit 1
              fi

              options=""
              command="$1"
              shift

              arguments=()
              while [ $# -gt 0 ]; do
                  case "$1" in
                      -o|--options) options="$2"; shift 2 ;;
                      *) arguments+=("$1"); shift ;;
                  esac
              done

              # Set Java home if available
              if [ -d "/opt/java/openjdk" ]; then
                export JAVA_HOME=/opt/java/openjdk
                export PATH=$JAVA_HOME/bin:$PATH
              fi

              # Export environment variables from --options, if provided
              if [ -n "$options" ]; then
                  IFS=',' read -ra env_array <<< "$options"
                  for env in "${env_array[@]}"; do
                      name="${env%%=*}"
                      value="${env#*=}"
                      value="${value%\"}"
                      value="${value#\"}"
                      value="${value%\'}"
                      value="${value#\'}"
                      export "$name=$value"
                  done
                  IFS="$old_ifs"
              fi

              # Execute starlake directly (shell mode, not docker mode)
              if [ -x "/shared-tools/starlake/starlake.sh" ]; then
                exec /shared-tools/starlake/starlake.sh "$command" "${arguments[@]}" 2>&1
              elif [ -x "/shared-tools/starlake/starlake" ]; then
                exec /shared-tools/starlake/starlake "$command" "${arguments[@]}" 2>&1
              elif [ -x "/usr/local/bin/starlake" ]; then
                exec /usr/local/bin/starlake "$command" "${arguments[@]}" 2>&1
              else
                echo "ERROR: starlake executable not found"
                exit 1
              fi
              EOFSL
              chmod +x /shared-tools/bin/starlake

              # gcloud wrapper (if SDK was copied)
              if [ -d "/shared-tools/gcloud" ]; then
                ln -sf /shared-tools/gcloud/google-cloud-sdk/bin/gcloud /shared-tools/bin/gcloud
                ln -sf /shared-tools/gcloud/google-cloud-sdk/bin/gsutil /shared-tools/bin/gsutil
              fi

              {{- if .Values.airflow.jobRunner.enabled }}
              # Install kubectl for Job Runner (must be done in initContainer as root)
              echo "Installing kubectl for Job Runner..."
              curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
              chmod +x kubectl
              mv kubectl /shared-tools/bin/kubectl
              echo "kubectl installed: $(/shared-tools/bin/kubectl version --client 2>/dev/null | head -1)"
              {{- end }}

              echo "Tools setup complete!"
              echo "Contents of /shared-tools:"
              ls -la /shared-tools/
              echo "Contents of /shared-tools/bin:"
              ls -la /shared-tools/bin/
              echo "Contents of /shared-tools/starlake:"
              ls -la /shared-tools/starlake/ || echo "No starlake dir"
          volumeMounts:
            - name: shared-tools
              mountPath: /shared-tools
        {{- end }}
        # Initialize Airflow DB directly in init container (more reliable than waiting for Job)
        - name: init-airflow-db
          image: "{{ .Values.airflow.image.repository }}:{{ .Values.airflow.image.tag }}"
          imagePullPolicy: {{ .Values.airflow.image.pullPolicy }}
          command:
            - /bin/bash
            - -c
            - |
              echo "Waiting for database to be ready..."
              sleep 5
              echo "Initializing Airflow database..."
              airflow db init || airflow db migrate
              echo "Creating Airflow admin user..."
              airflow users create \
                --username {{ .Values.airflow.admin.username }} \
                --firstname {{ .Values.airflow.admin.firstname }} \
                --lastname {{ .Values.airflow.admin.lastname }} \
                --role Admin \
                --email {{ .Values.airflow.admin.email }} \
                --password {{ .Values.airflow.admin.password }} || echo "User already exists"
              echo "Airflow database initialization complete!"
          env:
            - name: AIRFLOW__CORE__EXECUTOR
              value: {{ .Values.airflow.executor | quote }}
            - name: AIRFLOW__WEBSERVER__SECRET_KEY
              value: {{ .Values.airflow.secretKey | quote }}
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "starlake.postgresql.secretName" . }}
                  key: {{ include "starlake.postgresql.passwordKey" . }}
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: {{ include "starlake.postgresql.airflowConnectionString" . | quote }}
            - name: AIRFLOW__CORE__LOAD_EXAMPLES
              value: "false"
      containers:
        # Sidecar container that continuously fixes permissions on /projects
        # This is needed because Airflow subprocesses don't inherit umask from the main shell
        - name: permission-fixer
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              echo "Starting permission fixer sidecar..."
              while true; do
                # Fix group ownership and permissions on all files
                chgrp -R {{ .Values.podSecurityContext.fsGroup }} /projects 2>/dev/null || true
                chmod -R g+rwX /projects 2>/dev/null || true

                # DuckDB stored_secrets directories: remove group write (contains unencrypted credentials)
                find /projects -type d -name "stored_secrets" -exec chmod -R g-w {} \; 2>/dev/null || true

                # DuckDB secret files: owner-only (DuckDB requires 0600 for security)
                find /projects -name "*.duckdb_secret" -exec chmod 600 {} \; 2>/dev/null || true

                # Run every 5 seconds
                sleep 5
              done
          securityContext:
            runAsUser: 0
          resources:
            limits:
              cpu: 50m
              memory: 32Mi
            requests:
              cpu: 10m
              memory: 16Mi
          volumeMounts:
            - name: projects
              mountPath: /projects
        - name: airflow
          image: "{{ .Values.airflow.image.repository }}:{{ .Values.airflow.image.tag }}"
          imagePullPolicy: {{ .Values.airflow.image.pullPolicy }}
          {{- with .Values.securityContext }}
          securityContext:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            # Java environment for Starlake CLI
            - name: JAVA_HOME
              value: /opt/java/openjdk
            - name: AIRFLOW__CORE__EXECUTOR
              value: {{ .Values.airflow.executor | quote }}
            # Secret key must be the same across all Airflow components
            - name: AIRFLOW__WEBSERVER__SECRET_KEY
              value: {{ .Values.airflow.secretKey | quote }}
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "starlake.postgresql.secretName" . }}
                  key: {{ include "starlake.postgresql.passwordKey" . }}
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: {{ include "starlake.postgresql.airflowConnectionString" . | quote }}
            - name: AIRFLOW__CORE__LOAD_EXAMPLES
              value: "false"
            - name: AIRFLOW__API__AUTH_BACKENDS
              value: "airflow.api.auth.backend.basic_auth"
            - name: SL_HOME
              value: /app/starlake
            - name: AIRFLOW__WEBSERVER__BASE_URL
              {{- if .Values.airflow.baseUrl }}
              value: {{ .Values.airflow.baseUrl | quote }}
              {{- else }}
              value: {{ include "starlake.frontendUrl" . }}/airflow
              {{- end }}
            - name: AIRFLOW__CORE__LOGGING_LEVEL
              value: {{ .Values.airflow.logLevel | quote }}
            - name: AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL
              value: {{ .Values.airflow.dagDirListInterval | quote }}
            - name: AIRFLOW__DAG_PROCESSOR__MIN_FILE_PROCESS_INTERVAL
              value: {{ .Values.airflow.dagMinFileProcessInterval | quote }}
            {{- if .Values.airflow.jobRunner.enabled }}
            # Use starlake-k8s wrapper to execute tasks as Kubernetes Jobs
            - name: SL_STARLAKE_PATH
              value: "starlake-k8s"
            - name: STARLAKE_NAMESPACE
              value: {{ .Release.Namespace | quote }}
            # Add shared-tools/bin to PATH for kubectl
            - name: PATH
              value: "/shared-tools/bin:/opt/java/openjdk/bin:/home/airflow/.local/bin:/usr/local/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/sbin:/bin"
            {{- end }}
          command:
            - /bin/bash
            - -c
            - |
              # Set umask to allow group write on new files/directories
              umask 002

              echo "Waiting before starting Airflow..."
              sleep 10

              {{- if .Values.airflow.installPythonPackages }}
              echo "Installing Python packages (starlake-airflow for Airflow 2)..."
              # IMPORTANT: Pin starlake-airflow~=0.4 for Airflow 2 (0.5+ requires Airflow 3)
              pip install --no-cache-dir \
                "starlake-airflow>=0.4,<0.5" \
                docker \
                apache-airflow-providers-amazon \
                apache-airflow-providers-google
              {{- else }}
              echo "Skipping pip install (packages pre-installed in image)"
              {{- end }}

              # Note: kubectl is installed in initContainer (setup-starlake-tools) when jobRunner.enabled
              echo "Setting up Starlake tools in PATH..."
              # Add shared-tools/bin to PATH for starlake, gcloud, gsutil
              export PATH="/shared-tools/bin:$PATH"
              # If using custom image, starlake is already in /usr/local/bin
              if [ ! -f /usr/local/bin/starlake ]; then
                ln -sf /shared-tools/bin/starlake /usr/local/bin/starlake 2>/dev/null || true
                chmod +x /usr/local/bin/starlake 2>/dev/null || true
              fi

              echo "Starting Airflow webserver in background..."
              airflow webserver &

              echo "Starting Airflow scheduler..."
              exec airflow scheduler
          # Startup probe - allows slow startup (pip install + db init can take 2-3 min)
          # Matches docker-compose: interval=5s, retries=60 = 5 min max
          startupProbe:
            httpGet:
              path: /airflow/health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 60
          # Liveness probe - after startup succeeds
          livenessProbe:
            httpGet:
              path: /airflow/health
              port: http
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 6
          # Readiness probe
          readinessProbe:
            httpGet:
              path: /airflow/health
              port: http
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          resources:
            {{- toYaml .Values.airflow.webserver.resources | nindent 12 }}
          volumeMounts:
            - name: shared-tools
              mountPath: /shared-tools
            - name: projects
              mountPath: /projects
            - name: projects
              mountPath: /opt/airflow/dags
              subPath: dags
            {{- if .Values.airflow.logs.persistence.enabled }}
            - name: logs
              mountPath: /opt/airflow/logs
            {{- end }}
            {{- if .Values.airflow.jobRunner.enabled }}
            - name: job-template
              mountPath: /etc/starlake
            {{- if not .Values.airflow.jobRunner.useBuiltinWrapper }}
            - name: starlake-wrapper
              mountPath: /usr/local/bin/starlake-k8s
              subPath: starlake-k8s.sh
            {{- end }}
            {{- end }}
      volumes:
        - name: shared-tools
          emptyDir: {}
        - name: projects
          persistentVolumeClaim:
            claimName: {{ include "starlake.fullname" . }}-projects
        {{- if .Values.airflow.jobRunner.enabled }}
        - name: job-template
          configMap:
            name: {{ include "starlake.fullname" . }}-job-template
        {{- if not .Values.airflow.jobRunner.useBuiltinWrapper }}
        - name: starlake-wrapper
          configMap:
            name: {{ include "starlake.fullname" . }}-starlake-wrapper
            defaultMode: 0755
        {{- end }}
        {{- end }}
        {{- if .Values.airflow.logs.persistence.enabled }}
        - name: logs
          persistentVolumeClaim:
            claimName: {{ include "starlake.fullname" . }}-airflow-logs
        {{- else }}
        - name: logs
          emptyDir: {}
        {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
{{- end }}
