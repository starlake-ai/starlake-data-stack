{{- if and .Values.airflow.enabled .Values.airflow.jobRunner.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "starlake.fullname" . }}-job-template
  labels:
    {{- include "starlake.componentLabels" (dict "component" "airflow" "context" .) | nindent 4 }}
data:
  # Job template used by the starlake wrapper to create Kubernetes Jobs
  # Placeholders are replaced at runtime:
  #   __JOB_NAME__: unique job name (e.g., sl-load-customer-20240129-123456)
  #   __STARLAKE_COMMAND__: starlake command (e.g., load)
  #   __STARLAKE_ARGS__: command arguments as JSON array
  #   __ENV_VARS__: environment variables as JSON array
  job-template.yaml: |
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: __JOB_NAME__
      labels:
        app.kubernetes.io/name: starlake
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/component: starlake-job
        app.kubernetes.io/managed-by: airflow
    spec:
      ttlSecondsAfterFinished: {{ .Values.airflow.jobRunner.ttlSecondsAfterFinished }}
      backoffLimit: {{ .Values.airflow.jobRunner.backoffLimit }}
      template:
        metadata:
          labels:
            app.kubernetes.io/name: starlake
            app.kubernetes.io/instance: {{ .Release.Name }}
            app.kubernetes.io/component: starlake-job
        spec:
          restartPolicy: Never
          securityContext:
            fsGroup: {{ .Values.podSecurityContext.fsGroup }}
          containers:
            - name: starlake
              image: "{{ .Values.airflow.jobRunner.image.repository }}:{{ .Values.airflow.jobRunner.image.tag }}"
              imagePullPolicy: {{ .Values.airflow.jobRunner.image.pullPolicy }}
              # Use bash to create /incoming symlink before running starlake
              # NOTE: Must use /bin/bash because /app/starlake.sh has #!/usr/bin/env bash
              command:
                - /bin/bash
                - -c
                - |
                  # Create /incoming symlink pointing to $SL_ROOT/incoming
                  # Required because starlake projects use absolute /incoming path
                  if [ -n "$SL_ROOT" ]; then
                    mkdir -p "$SL_ROOT/incoming" 2>/dev/null || true
                    ln -sf "$SL_ROOT/incoming" /incoming 2>/dev/null || true
                    echo "- /incoming -> $SL_ROOT/incoming"
                  fi
                  # Execute starlake with the provided args
                  # /app/starlake.sh is the CLI location in the starlakeai/starlake image
                  exec /app/starlake.sh "$@"
                - "--"
              args: __STARLAKE_ARGS__
              env:
                # Storage Mode - ALWAYS local filesystem, even with SeaweedFS enabled
                # SL_ROOT is dynamically replaced by Airflow DAG with project path
                # SeaweedFS is provisioned but not used by default
                - name: SL_ROOT
                  value: __SL_ROOT__
                {{- if .Values.seaweedfs.enabled }}
                # S3 credentials available for manual project creation in UI
                # These are NOT used by default - only when user creates S3-backed projects
                - name: AWS_ACCESS_KEY_ID
                  value: {{ .Values.seaweedfs.s3.accessKey | quote }}
                - name: AWS_SECRET_ACCESS_KEY
                  value: {{ .Values.seaweedfs.s3.secretKey | quote }}
                - name: AWS_S3_ENDPOINT
                  value: "http://{{ include "starlake.fullname" . }}-seaweedfs:{{ .Values.seaweedfs.service.s3Port }}"
                - name: AWS_REGION
                  value: "us-east-1"
                - name: HADOOP_CONF_DIR
                  value: "/etc/hadoop/conf"
                {{- end }}
                __ENV_VARS__
              resources:
                requests:
                  memory: "{{ .Values.airflow.jobRunner.resources.requests.memory }}"
                  cpu: "{{ .Values.airflow.jobRunner.resources.requests.cpu }}"
                limits:
                  memory: "{{ .Values.airflow.jobRunner.resources.limits.memory }}"
                  cpu: "{{ .Values.airflow.jobRunner.resources.limits.cpu }}"
              volumeMounts:
                - name: projects
                  mountPath: /projects
                {{- if .Values.seaweedfs.enabled }}
                - name: hadoop-config
                  mountPath: /etc/hadoop/conf
                {{- end }}
          volumes:
            # Always use PVC for /projects (local filesystem)
            # SeaweedFS is provisioned separately but not used by default
            - name: projects
              persistentVolumeClaim:
                claimName: {{ include "starlake.fullname" . }}-projects
            {{- if .Values.seaweedfs.enabled }}
            - name: hadoop-config
              configMap:
                name: {{ include "starlake.fullname" . }}-hadoop-config
            {{- end }}
{{- end }}
